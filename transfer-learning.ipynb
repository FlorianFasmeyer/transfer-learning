{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df050ced-a017-4732-a1c6-a161194d6a7e",
   "metadata": {},
   "source": [
    "# A simple Transfer learning project\n",
    "\n",
    "In this project, we demonstrate the principle of transfert learning; The repurposing of existing neural networks for new projects.\n",
    "\n",
    "Deep neural networks can be expensive and time-consuming to train, with costs ranging from millions of dollars to processing times that can span months. However, the primary challenge lies in accessing the necessary data. Large companies, such as Google, often train their neural networks using proprietary data, which is closely guarded and constitutes the cornerstone of their flagship products, like Google's search engine.\n",
    "\n",
    "With the advent of transfer learning, willing corporations such as Google can share fully trained models without compromising their data. Existing networks can therefore be downloaded by the little guys and be cut, twisted and modified at will, saving tons of time and money!\n",
    "\n",
    "In this project, we will download EfficientNet B7, a convolutional network which, at the time, was known for its outstanding performance on computer vision tasks. The EfficientNet we donwload today was trained on datasets with thousands of labels, ranging from cats and dogs to cars and buildings.\n",
    "\n",
    "We will repurpose this instance of EfficientNet to work as a vehicle classification module.\n",
    "\n",
    "## The core principle\n",
    "The classical transfer learning method and the one we will use is to cut the classification tail of a network and replace it with another model.\n",
    "\n",
    "![Replacing the classification tail A with a neural network B.](./image/vgg-transfer-learning.png)\n",
    "\n",
    "The new appendage only needs a few seconds to a few hours of training to be operational. In this project, we will compare the use of two models, the K-NN and the Convolutional Network.\n",
    "\n",
    "**Terminology alert!**\n",
    "After removing the tail of our neural network, we call the last layer the \"feature map\" (see the above image). It is a bunch of raw neurons, each representing a high-level *feature*, or a concept. For instance, a given neuron's activation can represent the presence of fur-like texture in the image, which in addition with other neurons would indicate the presence of a cat in the picture.\n",
    "\n",
    "Now that we are done with the theory, **let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122c93d-bbc9-458a-9f5a-e5c7379b97ee",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "For all intents and purposes, all you need to know is that we separate our data into training, testing, and validation datasets, with `X` as the input (raw image: red, green, blue), and `y` as the label ('bike', 'car', 'truck', ...). *You may skip the rest of this section.*\n",
    "\n",
    "For curious readers, we retrive our data from a folder structured as such:\n",
    "\n",
    "![File structure for the project](./image/file-structure.png)\n",
    "\n",
    "The data is already segragated into train, test and validation sets, with each set containing a folder for each classe. It is an unusual way to store the data. To retrive our images from those folders we use...\n",
    "\n",
    "`os.path.join(*args)` to create a plateform agnostic filepath (Linux and Windows):\n",
    "\n",
    "![Join folder names into a full path](./image/mypath.png)\n",
    "\n",
    "`os.path.split(string)` to get the label, contained as the parent folder name of our images:\n",
    "\n",
    "![Split path name to keep the label](./image/split-image.png)\n",
    "\n",
    "`glob` to seach for all possible files, using the `*` wildcard:\n",
    "![Finding all files in data\\train\\bike](./image/wildcard-glob.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc606f9-aa59-4984-b19a-cfde5504ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import split, join\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324a1c31-e15d-4cac-b864-69067dc73d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(paths):\n",
    "    images = [np.array(Image.open(path)) for path in paths]\n",
    "    labels = [split(split(path)[0])[1] for path in paths]\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c74e78f-df80-41b1-bcd5-5cc866cc0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(glob(join('data', 'train', '*', '*')))\n",
    "X_test, y_test =   get_data(glob(join('data', 'test', '*', '*'))) \n",
    "X_valid, y_valid = get_data(glob(join('data', 'valid', '*', '*'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8ebf9-9fd0-4b31-b1ca-d460f7652302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c7d80-165f-40ff-a5da-452e58e61923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
